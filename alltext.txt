# =============================================================================
# FILE: scraper.py
# Main WebScraper class definition
# =============================================================================

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
import re
from typing import List, Dict, Set

class WebScraper:
    def __init__(self, delay: float = 1.0):
        """
        Initialize the web scraper.

        Args:
            delay: Delay between requests in seconds (be respectful to servers)
        """
        self.delay = delay
        self.session = requests.Session()
        # Set a user agent to avoid being blocked
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def fetch_page(self, url: str) -> BeautifulSoup:
        """
        Fetch and parse a web page.

        Args:
            url: The URL to fetch

        Returns:
            BeautifulSoup object of the parsed page
        """
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()  # Raise an exception for bad status codes

            # Parse the HTML content
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup

        except requests.RequestException as e:
            print(f"Error fetching {url}: {e}")
            return None

    def search_terms_in_text(self, text: str, search_terms: List[str], case_sensitive: bool = False) -> Dict[str, List[str]]:
        """
        Search for terms in text and return matches with context.

        Args:
            text: Text to search in
            search_terms: List of terms to search for
            case_sensitive: Whether search should be case sensitive

        Returns:
            Dictionary with terms as keys and list of context snippets as values
        """
        results = {}

        for term in search_terms:
            matches = []

            # Prepare text and term for searching
            search_text = text if case_sensitive else text.lower()
            search_term = term if case_sensitive else term.lower()

            # Find all occurrences
            pattern = re.escape(search_term)
            for match in re.finditer(pattern, search_text):
                start = max(0, match.start() - 50)  # 50 characters before
                end = min(len(text), match.end() + 50)  # 50 characters after
                context = text[start:end].strip()

                # Clean up the context
                context = re.sub(r'\s+', ' ', context)  # Replace multiple whitespace with single space
                matches.append(context)

            if matches:
                results[term] = matches

        return results

    def scrape_urls(self, urls: List[str], search_terms: List[str],
                   case_sensitive: bool = False, search_in: str = 'all') -> Dict[str, Dict]:
        """
        Scrape multiple URLs and search for terms.

        Args:
            urls: List of URLs to scrape
            search_terms: List of terms to search for
            case_sensitive: Whether search should be case sensitive
            search_in: Where to search - 'all', 'title', 'body', 'links'

        Returns:
            Dictionary with URLs as keys and search results as values
        """
        results = {}

        for i, url in enumerate(urls):
            print(f"Scraping {i+1}/{len(urls)}: {url}")

            # Add delay between requests
            if i > 0:
                time.sleep(self.delay)

            soup = self.fetch_page(url)
            if soup is None:
                results[url] = {"error": "Failed to fetch page"}
                continue

            # Extract text based on search_in parameter
            text_to_search = ""

            if search_in == 'all' or search_in == 'title':
                title = soup.find('title')
                if title:
                    text_to_search += title.get_text() + " "

            if search_in == 'all' or search_in == 'body':
                # Remove script and style elements
                for script in soup(["script", "style"]):
                    script.decompose()
                text_to_search += soup.get_text()

            if search_in == 'all' or search_in == 'links':
                links = soup.find_all('a')
                for link in links:
                    if link.get_text():
                        text_to_search += link.get_text() + " "
                    if link.get('href'):
                        text_to_search += link.get('href') + " "

            # Search for terms
            matches = self.search_terms_in_text(text_to_search, search_terms, case_sensitive)

            # Store results
            results[url] = {
                "matches": matches,
                "total_matches": sum(len(contexts) for contexts in matches.values()),
                "page_title": soup.find('title').get_text().strip() if soup.find('title') else "No title"
            }

        return results

    def print_results(self, results: Dict[str, Dict]):
        """
        Print search results in a formatted way.

        Args:
            results: Results dictionary from scrape_urls
        """
        print("\n" + "="*80)
        print("WEB SCRAPING RESULTS")
        print("="*80)

        for url, data in results.items():
            print(f"\nURL: {url}")

            if "error" in data:
                print(f"  ‚ùå {data['error']}")
                continue

            print(f"  üìÑ Title: {data['page_title']}")
            print(f"  üîç Total matches: {data['total_matches']}")

            if data['matches']:
                print("  üìù Found terms:")
                for term, contexts in data['matches'].items():
                    print(f"    ‚Ä¢ '{term}' ({len(contexts)} occurrences)")
                    for i, context in enumerate(contexts[:3], 1):  # Show max 3 contexts per term
                        print(f"      {i}. ...{context}...")
                    if len(contexts) > 3:
                        print(f"      ... and {len(contexts) - 3} more occurrences")
            else:
                print("  ‚ùå No matches found")


# =============================================================================
# FILE: config.py
# Configuration settings for the scraper
# =============================================================================

# URLs to scrape
URLS = [
    "https://httpbin.org/html",
    "https://example.com",
    "https://python.org",
    "https://docs.python.org/3/",
    "https://realpython.com"
]

# Terms to search for
SEARCH_TERMS = [
    "python",
    "programming",
    "example",
    "HTTP",
    "web",
    "development"
]

# Scraper settings
SCRAPER_DELAY = 1.0  # Delay between requests in seconds
CASE_SENSITIVE = False
SEARCH_IN = 'all'  # Options: 'all', 'title', 'body', 'links'


# =============================================================================
# FILE: main.py
# Main execution script
# =============================================================================

from scraper import WebScraper
from config import URLS, SEARCH_TERMS, SCRAPER_DELAY, CASE_SENSITIVE, SEARCH_IN

def main():
    """Main function to run the web scraper."""
    print("üîç Web Scraper Starting...")
    print(f"üìù URLs to scrape: {len(URLS)}")
    print(f"üîé Search terms: {SEARCH_TERMS}")
    print(f"‚è±Ô∏è  Delay between requests: {SCRAPER_DELAY}s")
    print(f"üî§ Case sensitive: {CASE_SENSITIVE}")
    print(f"üìç Search in: {SEARCH_IN}")
    print("-" * 50)

    # Initialize the scraper
    scraper = WebScraper(delay=SCRAPER_DELAY)

    # Scrape and search
    results = scraper.scrape_urls(
        urls=URLS,
        search_terms=SEARCH_TERMS,
        case_sensitive=CASE_SENSITIVE,
        search_in=SEARCH_IN
    )

    # Print results
    scraper.print_results(results)

    # Optional: Save results to file
    save_results_to_file(results)

def save_results_to_file(results):
    """Save results to a text file."""
    import json
    from datetime import datetime

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"scraper_results_{timestamp}.json"

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ Results saved to: {filename}")
    except Exception as e:
        print(f"\n‚ùå Error saving results: {e}")

if __name__ == "__main__":
    main()


# =============================================================================
# FILE: test_scraper.py
# Unit tests for the web scraper
# =============================================================================

import unittest
from scraper import WebScraper
from bs4 import BeautifulSoup

class TestWebScraper(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures."""
        self.scraper = WebScraper(delay=0.1)  # Short delay for testing

    def test_search_terms_in_text(self):
        """Test the search functionality."""
        text = "This is a test text about Python programming and web development."
        search_terms = ["python", "web", "nonexistent"]

        results = self.scraper.search_terms_in_text(text, search_terms, case_sensitive=False)

        self.assertIn("python", results)
        self.assertIn("web", results)
        self.assertNotIn("nonexistent", results)
        self.assertEqual(len(results["python"]), 1)
        self.assertEqual(len(results["web"]), 1)

    def test_case_sensitive_search(self):
        """Test case sensitive search."""
        text = "Python is different from python"
        search_terms = ["Python"]

        # Case sensitive
        results_sensitive = self.scraper.search_terms_in_text(text, search_terms, case_sensitive=True)
        self.assertEqual(len(results_sensitive["Python"]), 1)

        # Case insensitive
        results_insensitive = self.scraper.search_terms_in_text(text, search_terms, case_sensitive=False)
        self.assertEqual(len(results_insensitive["Python"]), 2)

    def test_fetch_page_invalid_url(self):
        """Test handling of invalid URLs."""
        result = self.scraper.fetch_page("https://this-url-definitely-does-not-exist-12345.com")
        self.assertIsNone(result)

if __name__ == "__main__":
    unittest.main()


# =============================================================================
# FILE: example_usage.py
# Examples of how to use the web scraper
# =============================================================================

from scraper import WebScraper

def example_basic_usage():
    """Example of basic scraper usage."""
    print("=== EXAMPLE 1: Basic Usage ===")

    scraper = WebScraper(delay=1.0)

    urls = ["https://example.com", "https://httpbin.org/html"]
    search_terms = ["example", "html"]

    results = scraper.scrape_urls(urls, search_terms)
    scraper.print_results(results)

def example_title_only_search():
    """Example of searching only in page titles."""
    print("\n=== EXAMPLE 2: Title-Only Search ===")

    scraper = WebScraper(delay=1.0)

    urls = ["https://python.org", "https://github.com"]
    search_terms = ["python", "github"]

    results = scraper.scrape_urls(
        urls=urls,
        search_terms=search_terms,
        search_in='title'
    )
    scraper.print_results(results)

def example_case_sensitive():
    """Example of case-sensitive search."""
    print("\n=== EXAMPLE 3: Case-Sensitive Search ===")

    scraper = WebScraper(delay=1.0)

    urls = ["https://python.org"]
    search_terms = ["Python", "PYTHON"]  # Different cases

    results = scraper.scrape_urls(
        urls=urls,
        search_terms=search_terms,
        case_sensitive=True
    )
    scraper.print_results(results)

def example_custom_search():
    """Example with custom URLs and terms."""
    print("\n=== EXAMPLE 4: Custom Search ===")

    # You can customize these for your specific needs
    custom_urls = [
        "https://news.ycombinator.com",
        "https://stackoverflow.com/questions/tagged/python"
    ]

    custom_terms = ["javascript", "python", "react"]

    scraper = WebScraper(delay=2.0)  # Longer delay for politeness

    results = scraper.scrape_urls(
        urls=custom_urls,
        search_terms=custom_terms,
        case_sensitive=False,
        search_in='all'
    )

    scraper.print_results(results)

if __name__ == "__main__":
    # Run all examples
    example_basic_usage()
    example_title_only_search()
    example_case_sensitive()
    example_custom_search()


# =============================================================================
# FILE: requirements.txt
# Dependencies for the project
# =============================================================================

# Web scraping dependencies
requests==2.31.0
beautifulsoup4==4.12.2

# Optional: for better HTML parsing
lxml==4.9.3

# Optional: for progress bars
tqdm==4.66.1


# =============================================================================
# FILE: README.md
# Project documentation
# =============================================================================

"""
# Web Scraper Project

A simple, respectful web scraper built with Python and BeautifulSoup.

## Installation

1. Install Python 3.7 or higher
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Quick Start

Run the main script:
```bash
python main.py
```

## File Structure

- `scraper.py` - Main WebScraper class
- `config.py` - Configuration settings
- `main.py` - Main execution script
- `test_scraper.py` - Unit tests
- `example_usage.py` - Usage examples
- `requirements.txt` - Dependencies
- `README.md` - This file

## Usage Examples

### Basic Usage
```python
from scraper import WebScraper

scraper = WebScraper(delay=1.0)
results = scraper.scrape_urls(
    urls=["https://example.com"],
    search_terms=["example", "test"]
)
scraper.print_results(results)
```

### Search Only in Titles
```python
results = scraper.scrape_urls(
    urls=urls,
    search_terms=terms,
    search_in='title'
)
```

## Running Tests

```bash
python test_scraper.py
```

## Configuration

Edit `config.py` to customize:
- URLs to scrape
- Search terms
- Scraper settings (delay, case sensitivity, etc.)

## Important Notes

- Always respect robots.txt and website terms of service
- The scraper includes delays between requests to be respectful
- Some websites may block automated requests
- Consider legal and ethical implications of web scraping

## License

This project is for educational purposes. Use responsibly.
"""